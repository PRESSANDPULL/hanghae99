{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOtXq/IPjR2ak5jGoVRyRUB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PRESSANDPULL/hanghae99/blob/main/advanded_CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [심화과제] CIFAR10\n",
        "\n",
        "기본과제와 마찬가지로 개발 관련 기본 지식이 부족하여 학습하는 속도가 과제를 따라갈 수 없어서 과제는 우선 ChatGPT를 활용하였습니다.."
      ],
      "metadata": {
        "id": "auP-sYT2cjwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##\t1.\tDataset 변경:\n",
        "torchvision.datasets.MNIST를 torchvision.datasets.CIFAR10으로 변경.\n",
        "CIFAR10 데이터셋의 입력 shape은 **[3, 32, 32]**이므로, 이를 모델의 input_dim으로 반영.\n",
        "##\t2.\tActivation 함수 변경:\n",
        "모델에서 nn.ReLU를 nn.LeakyReLU로 변경."
      ],
      "metadata": {
        "id": "jqfpcRbIdX9D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyywozitchqZ",
        "outputId": "0426495f-7d74-4c63-a2a7-d4efed92b02d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:18<00:00, 9.01MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn  # nn 모듈 import\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# CIFAR10 데이터셋 로드\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# DataLoader 생성\n",
        "batch_size = 256\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Activation 함수 변경\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_dim, n_dim):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, n_dim)\n",
        "        self.layer2 = nn.Linear(n_dim, n_dim)\n",
        "        self.layer3 = nn.Linear(n_dim, 10)  # CIFAR10은 10개의 클래스\n",
        "        self.act = nn.LeakyReLU()  # Leaky ReLU 적용\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.act(self.layer1(x))\n",
        "        x = self.act(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. CIFAR10 입력 Shape 확인"
      ],
      "metadata": {
        "id": "YhWqmkiHdjgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR10 입력 shape 확인\n",
        "print(\"Trainset size:\", len(trainset))\n",
        "print(\"Image shape:\", trainset[0][0].shape)  # (3, 32, 32)\n",
        "\n",
        "# input_dim 설정\n",
        "input_dim = 3 * 32 * 32\n",
        "model = Model(input_dim, 1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvU0BNXIdmjn",
        "outputId": "0e8b7893-5b37-4334-b6da-f11d2cee8d6a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainset size: 50000\n",
            "Image shape: torch.Size([3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. SGD와 Adam 성능 비교"
      ],
      "metadata": {
        "id": "lBILRxaMdp0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import SGD, Adam  # SGD와 Adam 모두 import\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 손실 함수 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# accuracy 함수 정의\n",
        "def accuracy(model, dataloader):\n",
        "    model.eval()  # 평가 모드\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # 기울기 계산 비활성화\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')  # 데이터 이동\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)  # 가장 높은 확률의 클래스 선택\n",
        "            correct += (preds == labels).sum().item()  # 올바르게 예측한 샘플 수\n",
        "            total += labels.size(0)  # 전체 샘플 수\n",
        "\n",
        "    return correct / total  # 정확도 계산\n",
        "\n",
        "# SGD와 Adam Optimizer 정의\n",
        "optimizer_sgd = SGD(model.parameters(), lr=0.001)\n",
        "optimizer_adam = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# SGD와 Adam 각각의 학습 정확도 저장\n",
        "train_accs_sgd = []\n",
        "train_accs_adam = []\n",
        "\n",
        "# 모델을 GPU로 이동\n",
        "model = model.to('cuda')\n",
        "\n",
        "# 학습 루프\n",
        "for epoch in range(50):  # n_epochs = 50\n",
        "    print(f\"Epoch {epoch + 1}/50\")  # 에포크 시작 출력\n",
        "\n",
        "    # SGD 학습\n",
        "    model.train()\n",
        "    running_loss_sgd = 0.0  # SGD 학습 손실 저장\n",
        "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "        optimizer_sgd.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_sgd.step()\n",
        "        running_loss_sgd += loss.item()\n",
        "\n",
        "        # 10번째 배치마다 출력\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"  [SGD] Batch {batch_idx + 1}/{len(trainloader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    train_acc_sgd = accuracy(model, trainloader)\n",
        "    train_accs_sgd.append(train_acc_sgd)\n",
        "    print(f\"  [SGD] Epoch {epoch + 1} Loss: {running_loss_sgd / len(trainloader):.4f}, Train Accuracy: {train_acc_sgd:.4f}\")\n",
        "\n",
        "    # Adam 학습\n",
        "    model.train()\n",
        "    running_loss_adam = 0.0  # Adam 학습 손실 저장\n",
        "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "        optimizer_adam.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_adam.step()\n",
        "        running_loss_adam += loss.item()\n",
        "\n",
        "        # 10번째 배치마다 출력\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"  [Adam] Batch {batch_idx + 1}/{len(trainloader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    train_acc_adam = accuracy(model, trainloader)\n",
        "    train_accs_adam.append(train_acc_adam)\n",
        "    print(f\"  [Adam] Epoch {epoch + 1} Loss: {running_loss_adam / len(trainloader):.4f}, Train Accuracy: {train_acc_adam:.4f}\")\n",
        "\n",
        "# SGD와 Adam 성능 비교 그래프\n",
        "plt.plot(range(50), train_accs_sgd, label=\"SGD\")\n",
        "plt.plot(range(50), train_accs_adam, label=\"Adam\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Train Accuracy\")\n",
        "plt.title(\"SGD vs Adam: Train Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z7PPnCudtpE",
        "outputId": "3ddbeaee-1164-478b-e2cc-53ce4da829ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "  [SGD] Batch 10/196, Loss: 0.7124\n",
            "  [SGD] Batch 20/196, Loss: 0.7665\n",
            "  [SGD] Batch 30/196, Loss: 0.6415\n",
            "  [SGD] Batch 40/196, Loss: 0.6817\n",
            "  [SGD] Batch 50/196, Loss: 0.6786\n",
            "  [SGD] Batch 60/196, Loss: 0.6019\n",
            "  [SGD] Batch 70/196, Loss: 0.7321\n",
            "  [SGD] Batch 80/196, Loss: 0.6753\n",
            "  [SGD] Batch 90/196, Loss: 0.6989\n",
            "  [SGD] Batch 100/196, Loss: 0.7100\n",
            "  [SGD] Batch 110/196, Loss: 0.6562\n",
            "  [SGD] Batch 120/196, Loss: 0.5955\n",
            "  [SGD] Batch 130/196, Loss: 0.6661\n",
            "  [SGD] Batch 140/196, Loss: 0.6245\n",
            "  [SGD] Batch 150/196, Loss: 0.6903\n",
            "  [SGD] Batch 160/196, Loss: 0.6102\n",
            "  [SGD] Batch 170/196, Loss: 0.6907\n",
            "  [SGD] Batch 180/196, Loss: 0.6991\n",
            "  [SGD] Batch 190/196, Loss: 0.7306\n",
            "  [SGD] Epoch 1 Loss: 0.6716, Train Accuracy: 0.7809\n",
            "  [Adam] Batch 10/196, Loss: 0.7690\n",
            "  [Adam] Batch 20/196, Loss: 0.8396\n",
            "  [Adam] Batch 30/196, Loss: 0.7708\n",
            "  [Adam] Batch 40/196, Loss: 0.6537\n",
            "  [Adam] Batch 50/196, Loss: 0.7021\n",
            "  [Adam] Batch 60/196, Loss: 0.6691\n",
            "  [Adam] Batch 70/196, Loss: 0.7141\n",
            "  [Adam] Batch 80/196, Loss: 0.7077\n",
            "  [Adam] Batch 90/196, Loss: 0.6583\n",
            "  [Adam] Batch 100/196, Loss: 0.7453\n",
            "  [Adam] Batch 110/196, Loss: 0.6837\n",
            "  [Adam] Batch 120/196, Loss: 0.7494\n",
            "  [Adam] Batch 130/196, Loss: 0.7554\n",
            "  [Adam] Batch 140/196, Loss: 0.6338\n",
            "  [Adam] Batch 150/196, Loss: 0.6238\n",
            "  [Adam] Batch 160/196, Loss: 0.7533\n",
            "  [Adam] Batch 170/196, Loss: 0.6615\n",
            "  [Adam] Batch 180/196, Loss: 0.8573\n",
            "  [Adam] Batch 190/196, Loss: 0.7372\n",
            "  [Adam] Epoch 1 Loss: 0.7201, Train Accuracy: 0.8113\n",
            "Epoch 2/50\n",
            "  [SGD] Batch 10/196, Loss: 0.5283\n",
            "  [SGD] Batch 20/196, Loss: 0.4926\n",
            "  [SGD] Batch 30/196, Loss: 0.5588\n",
            "  [SGD] Batch 40/196, Loss: 0.5199\n",
            "  [SGD] Batch 50/196, Loss: 0.4807\n",
            "  [SGD] Batch 60/196, Loss: 0.5134\n",
            "  [SGD] Batch 70/196, Loss: 0.4773\n",
            "  [SGD] Batch 80/196, Loss: 0.5278\n",
            "  [SGD] Batch 90/196, Loss: 0.5592\n",
            "  [SGD] Batch 100/196, Loss: 0.5758\n",
            "  [SGD] Batch 110/196, Loss: 0.5895\n",
            "  [SGD] Batch 120/196, Loss: 0.5750\n",
            "  [SGD] Batch 130/196, Loss: 0.5091\n",
            "  [SGD] Batch 140/196, Loss: 0.5029\n",
            "  [SGD] Batch 150/196, Loss: 0.5380\n",
            "  [SGD] Batch 160/196, Loss: 0.5096\n",
            "  [SGD] Batch 170/196, Loss: 0.4724\n",
            "  [SGD] Batch 180/196, Loss: 0.5166\n",
            "  [SGD] Batch 190/196, Loss: 0.5028\n",
            "  [SGD] Epoch 2 Loss: 0.5258, Train Accuracy: 0.8337\n",
            "  [Adam] Batch 10/196, Loss: 0.6010\n",
            "  [Adam] Batch 20/196, Loss: 0.5157\n",
            "  [Adam] Batch 30/196, Loss: 0.5122\n",
            "  [Adam] Batch 40/196, Loss: 0.5475\n",
            "  [Adam] Batch 50/196, Loss: 0.5922\n",
            "  [Adam] Batch 60/196, Loss: 0.6871\n",
            "  [Adam] Batch 70/196, Loss: 0.5467\n",
            "  [Adam] Batch 80/196, Loss: 0.5699\n",
            "  [Adam] Batch 90/196, Loss: 0.5222\n",
            "  [Adam] Batch 100/196, Loss: 0.6847\n",
            "  [Adam] Batch 110/196, Loss: 0.6686\n",
            "  [Adam] Batch 120/196, Loss: 0.7082\n",
            "  [Adam] Batch 130/196, Loss: 0.5164\n",
            "  [Adam] Batch 140/196, Loss: 0.5920\n",
            "  [Adam] Batch 150/196, Loss: 0.5039\n",
            "  [Adam] Batch 160/196, Loss: 0.6745\n",
            "  [Adam] Batch 170/196, Loss: 0.6223\n",
            "  [Adam] Batch 180/196, Loss: 0.7385\n",
            "  [Adam] Batch 190/196, Loss: 0.6971\n",
            "  [Adam] Epoch 2 Loss: 0.5925, Train Accuracy: 0.8359\n",
            "Epoch 3/50\n",
            "  [SGD] Batch 10/196, Loss: 0.4758\n",
            "  [SGD] Batch 20/196, Loss: 0.4755\n",
            "  [SGD] Batch 30/196, Loss: 0.5079\n",
            "  [SGD] Batch 40/196, Loss: 0.5131\n",
            "  [SGD] Batch 50/196, Loss: 0.4849\n",
            "  [SGD] Batch 60/196, Loss: 0.4068\n",
            "  [SGD] Batch 70/196, Loss: 0.4936\n",
            "  [SGD] Batch 80/196, Loss: 0.4651\n",
            "  [SGD] Batch 90/196, Loss: 0.3495\n",
            "  [SGD] Batch 100/196, Loss: 0.3932\n",
            "  [SGD] Batch 110/196, Loss: 0.4562\n",
            "  [SGD] Batch 120/196, Loss: 0.4769\n",
            "  [SGD] Batch 130/196, Loss: 0.4190\n",
            "  [SGD] Batch 140/196, Loss: 0.4334\n",
            "  [SGD] Batch 150/196, Loss: 0.4287\n",
            "  [SGD] Batch 160/196, Loss: 0.4175\n",
            "  [SGD] Batch 170/196, Loss: 0.3853\n",
            "  [SGD] Batch 180/196, Loss: 0.4206\n",
            "  [SGD] Batch 190/196, Loss: 0.5189\n",
            "  [SGD] Epoch 3 Loss: 0.4581, Train Accuracy: 0.8601\n",
            "  [Adam] Batch 10/196, Loss: 0.3787\n",
            "  [Adam] Batch 20/196, Loss: 0.4205\n",
            "  [Adam] Batch 30/196, Loss: 0.4418\n",
            "  [Adam] Batch 40/196, Loss: 0.4686\n",
            "  [Adam] Batch 50/196, Loss: 0.4448\n",
            "  [Adam] Batch 60/196, Loss: 0.4686\n",
            "  [Adam] Batch 70/196, Loss: 0.4752\n",
            "  [Adam] Batch 80/196, Loss: 0.4243\n",
            "  [Adam] Batch 90/196, Loss: 0.4729\n",
            "  [Adam] Batch 100/196, Loss: 0.5135\n",
            "  [Adam] Batch 110/196, Loss: 0.4969\n",
            "  [Adam] Batch 120/196, Loss: 0.4806\n",
            "  [Adam] Batch 130/196, Loss: 0.4519\n",
            "  [Adam] Batch 140/196, Loss: 0.5192\n",
            "  [Adam] Batch 150/196, Loss: 0.5125\n",
            "  [Adam] Batch 160/196, Loss: 0.6400\n",
            "  [Adam] Batch 170/196, Loss: 0.4456\n",
            "  [Adam] Batch 180/196, Loss: 0.5771\n",
            "  [Adam] Batch 190/196, Loss: 0.6478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Leaky ReLU와 Sigmoid 성능 비교"
      ],
      "metadata": {
        "id": "Y5Bq41d7dwKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid를 사용하는 모델 클래스 정의\n",
        "class SigmoidModel(Model):\n",
        "    def __init__(self, input_dim, n_dim):\n",
        "        super().__init__(input_dim, n_dim)\n",
        "        self.act = nn.Sigmoid()  # Sigmoid 적용\n",
        "\n",
        "# Sigmoid 모델 학습\n",
        "sigmoid_model = SigmoidModel(input_dim, 1024).to('cuda')\n",
        "optimizer_sigmoid = Adam(sigmoid_model.parameters(), lr=0.001)\n",
        "train_accs_sigmoid = []\n",
        "\n",
        "for epoch in range(50):\n",
        "    print(f\"Epoch {epoch + 1}/50 [Sigmoid Model]\")  # 에포크 출력\n",
        "    sigmoid_model.train()\n",
        "    running_loss_sigmoid = 0.0  # 에포크 손실 저장\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "        optimizer_sigmoid.zero_grad()\n",
        "        outputs = sigmoid_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_sigmoid.step()\n",
        "        running_loss_sigmoid += loss.item()\n",
        "\n",
        "        # 10번째 배치마다 출력\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"  Batch {batch_idx + 1}/{len(trainloader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    train_acc = accuracy(sigmoid_model, trainloader)\n",
        "    train_accs_sigmoid.append(train_acc)\n",
        "\n",
        "    # 에포크별 손실과 정확도 출력\n",
        "    print(f\"  Epoch {epoch + 1} Loss: {running_loss_sigmoid / len(trainloader):.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "# Leaky ReLU와 Sigmoid 성능 비교 그래프\n",
        "plt.plot(range(50), train_accs_adam, label=\"Leaky ReLU (Adam)\")\n",
        "plt.plot(range(50), train_accs_sigmoid, label=\"Sigmoid (Adam)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Train Accuracy\")\n",
        "plt.title(\"Leaky ReLU vs Sigmoid: Train Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h82OVhJGdy3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Dropout 적용 후 Generalization Error 확인"
      ],
      "metadata": {
        "id": "0G7rIEYQd1DS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropout이 포함된 모델 정의\n",
        "class DropoutModel(Model):\n",
        "    def __init__(self, input_dim, n_dim):\n",
        "        super().__init__(input_dim, n_dim)\n",
        "        self.dropout = nn.Dropout(0.1)  # Dropout 확률 0.1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.dropout(self.act(self.layer1(x)))\n",
        "        x = self.dropout(self.act(self.layer2(x)))\n",
        "        x = self.layer3(x)\n",
        "        return x\n",
        "\n",
        "# Dropout 모델 학습\n",
        "dropout_model = DropoutModel(input_dim, 1024).to('cuda')\n",
        "optimizer_dropout = Adam(dropout_model.parameters(), lr=0.001)\n",
        "train_accs_dropout = []\n",
        "test_accs_dropout = []\n",
        "\n",
        "for epoch in range(50):\n",
        "    print(f\"Epoch {epoch + 1}/50 [Dropout Model]\")  # 에포크 시작 출력\n",
        "    dropout_model.train()\n",
        "    running_loss_dropout = 0.0  # 에포크 손실 저장\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "        optimizer_dropout.zero_grad()\n",
        "        outputs = dropout_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_dropout.step()\n",
        "        running_loss_dropout += loss.item()\n",
        "\n",
        "        # 10번째 배치마다 손실 출력\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"  Batch {batch_idx + 1}/{len(trainloader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # 학습 및 테스트 정확도 계산\n",
        "    train_acc = accuracy(dropout_model, trainloader)\n",
        "    test_acc = accuracy(dropout_model, testloader)\n",
        "    train_accs_dropout.append(train_acc)\n",
        "    test_accs_dropout.append(test_acc)\n",
        "\n",
        "    # 에포크 손실 및 정확도 출력\n",
        "    print(f\"  Epoch {epoch + 1} Loss: {running_loss_dropout / len(trainloader):.4f}\")\n",
        "    print(f\"  Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Dropout 적용 전후 성능 비교 그래프\n",
        "plt.plot(range(50), train_accs_dropout, label=\"Train Accuracy (Dropout)\")\n",
        "plt.plot(range(50), test_accs_dropout, label=\"Test Accuracy (Dropout)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Dropout: Train vs Test Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P9xVL1S4d3Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iSpIXyD7j_V2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}